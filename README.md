# Distributed Log Analysis System ðŸš€

**Author:** Saurav Dani  
**Tech Stack:** Hadoop HDFS, PySpark, Apache Airflow, Python, Machine Learning (Isolation Forest/Z-Score)

---

## ðŸ“– Project Overview

This project is an end-to-end **Big Data Pipeline** designed to ingest, process, and analyze massive volumes of system logs from diverse sources (Hadoop, Apache, Android, etc.).

It solves the problem of "Log Fatigue" by automating the transformation of unstructured text logs into structured insights. The system features a **Universal Ingestion Engine**, a **Real-Time Anomaly Detection System**, and a **Live Data Simulator** to mimic high-velocity server traffic.

---

## ðŸ— Architecture (Medallion Pattern)

The data flows through a structured **Data Lake** architecture on HDFS:

1.  **Bronze Layer (Raw)**:  
    * **Input:** Raw text logs (`.log`) from 16+ different sources.
    * **Storage:** `/user/talentum/project_logs/raw/`
    * **Process:** Direct ingestion via `hdfs dfs -put` or Live Stream Generator.

2.  **Silver Layer (Refined)**:  
    * **Transformation:** Parsing unstructured text using **Regex** into structured columns (`timestamp`, `level`, `component`, `message`).
    * **Storage:** `/user/talentum/project_logs/refined/` (Parquet format).
    * **Engine:** PySpark Ingestion Job.

3.  **Gold Layer (Curated)**:  
    * **Transformation:** Aggregating metrics (Error Rates, Warning Counts) over time windows (e.g., 1 minute).
    * **Storage:** `/user/talentum/project_logs/curated/`
    * **Analytics:** Anomaly Detection (Statistical Z-Score Analysis) to flag spikes in error rates.

---

## ðŸ“‚ Project Structure

```text
Distributed-log-analyzer/
â”‚
â”œâ”€â”€ config/                     # Configuration Center
â”‚   â”œâ”€â”€ schemas.py              # Regex patterns for 16+ log types
â”‚   â””â”€â”€ etl_config.yaml         # Central paths and thresholds
â”‚
â”œâ”€â”€ dags/                       # Automation
â”‚   â”œâ”€â”€ log_ingestion_dag.py    # Airflow DAG for batch processing
â”‚   â””â”€â”€ live_stream_dag.py      # Airflow DAG for continuous live streaming
â”‚
â”œâ”€â”€ spark_jobs/                 # Core Processing Logic
â”‚   â”œâ”€â”€ common/                 # Shared utilities (SparkSession, Config Loader)
â”‚   â”œâ”€â”€ ingestion/              # Bronze -> Silver logic
â”‚   â”‚   â””â”€â”€ universal_ingest.py # The Generic Parser
â”‚   â””â”€â”€ analysis/               # Silver -> Gold logic
â”‚       â”œâ”€â”€ calculate_trends.py # Aggregation Engine
â”‚       â””â”€â”€ detect_anomalies.py # Machine Learning Engine
â”‚
â”œâ”€â”€ scripts/                    # Helper Scripts
â”‚   â”œâ”€â”€ setup_hdfs.sh           # Builds the Data Lake folders
â”‚   â”œâ”€â”€ upload_samples.sh       # Pushes local data to HDFS
â”‚   â””â”€â”€ generate_live_logs.py   # Simulates a live server attack
â”‚
â”œâ”€â”€ notebooks/                  # Interactive Development
â”‚   â”œâ”€â”€ Ingestion_Layer.ipynb   # Dev notebook for parsing
â”‚   â”œâ”€â”€ Calculate_Trends.ipynb  # Dev notebook for aggregation
â”‚   â””â”€â”€ Anomaly_Detection.ipynb # Dev notebook for ML
â”‚
â””â”€â”€ data/                       # Local Landing Zone for logs
