{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f14261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for pre-installed Spark...\n",
      "--> Found Spark at: /home/talentum/spark\n",
      "--> Added Py4J: /home/talentum/spark/python/lib/py4j-0.10.7-src.zip\n",
      "\n",
      "SUCCESS: All modules loaded correctly! You can now run the next cell.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ====================================================\n",
    "# FIX: AUTO-DETECT SPARK ON LOCAL MACHINE (No PIP needed)\n",
    "# ====================================================\n",
    "\n",
    "# 1. Unset the conflicting variable inside Python to prevent future crashes\n",
    "if \"PYTHONPATH\" in os.environ:\n",
    "    del os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "# 2. Define common places where Spark lives in training VMs\n",
    "candidate_paths = [\n",
    "    os.environ.get(\"SPARK_HOME\"), \n",
    "    \"/usr/local/spark\",\n",
    "    \"/usr/lib/spark\",\n",
    "    \"/home/talentum/spark\",\n",
    "    \"/opt/spark\",\n",
    "    \"/usr/hdp/current/spark2-client\"\n",
    "]\n",
    "\n",
    "SPARK_HOME = None\n",
    "\n",
    "# 3. Search for the folder\n",
    "print(\"Searching for pre-installed Spark...\")\n",
    "for path in candidate_paths:\n",
    "    if path and os.path.exists(path) and os.path.exists(os.path.join(path, \"python\")):\n",
    "        SPARK_HOME = path\n",
    "        print(f\"--> Found Spark at: {SPARK_HOME}\")\n",
    "        break\n",
    "\n",
    "# 4. Add to Python Path if found\n",
    "if SPARK_HOME:\n",
    "    # Add the python folder (where pyspark lives)\n",
    "    sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))\n",
    "    \n",
    "    # Add the py4j zip file (required for Java communication)\n",
    "    py4j_files = glob.glob(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-*-src.zip\"))\n",
    "    if py4j_files:\n",
    "        sys.path.insert(0, py4j_files[0])\n",
    "        print(f\"--> Added Py4J: {py4j_files[0]}\")\n",
    "    else:\n",
    "        print(\"--> Warning: Could not find py4j zip file.\")\n",
    "else:\n",
    "    print(\"--> Error: Could not find Spark folder.\")\n",
    "\n",
    "PROJECT_ROOT = \"/home/talentum/Distributed-log-analyzer\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Now try the imports\n",
    "try:\n",
    "    from spark_jobs.common.spark_utils import get_spark_session, load_config\n",
    "    import config.schemas as schemas\n",
    "    from pyspark.sql.functions import input_file_name, current_timestamp, regexp_extract, col\n",
    "    print(\"\\nSUCCESS: All modules loaded correctly! You can now run the next cell.\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nFAILURE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37af9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Source: Hadoop ---\n",
      "Reading from: /user/talentum/project_logs/raw/Hadoop\n",
      "Pattern: ^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) (\\S+) \\[(.*?)\\] (.*?): (.*)\n",
      "Raw Count: 2000 rows\n",
      "--- Data Preview ---\n",
      "+-----------------------+-----+------+----------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-------------------------------------------------------------------------+\n",
      "|timestamp              |level|thread|component                                     |message                                                                                                                                               |ingest_time            |source_file                                                              |\n",
      "+-----------------------+-----+------+----------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-------------------------------------------------------------------------+\n",
      "|2015-10-18 18:01:47,978|INFO |main  |org.apache.hadoop.mapreduce.v2.app.MRAppMaster|Created MRAppMaster for application appattempt_1445144423722_0020_000001                                                                              |2026-02-07 18:56:49.691|hdfs://localhost:9000/user/talentum/project_logs/raw/Hadoop/Hadoop_2k.log|\n",
      "|2015-10-18 18:01:48,963|INFO |main  |org.apache.hadoop.mapreduce.v2.app.MRAppMaster|Executing with tokens:                                                                                                                                |2026-02-07 18:56:49.691|hdfs://localhost:9000/user/talentum/project_logs/raw/Hadoop/Hadoop_2k.log|\n",
      "|2015-10-18 18:01:48,963|INFO |main  |org.apache.hadoop.mapreduce.v2.app.MRAppMaster|Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 20 cluster_timestamp: 1445144423722 } attemptId: 1 } keyId: -127633188)|2026-02-07 18:56:49.691|hdfs://localhost:9000/user/talentum/project_logs/raw/Hadoop/Hadoop_2k.log|\n",
      "|2015-10-18 18:01:49,228|INFO |main  |org.apache.hadoop.mapreduce.v2.app.MRAppMaster|Using mapred newApiCommitter.                                                                                                                         |2026-02-07 18:56:49.691|hdfs://localhost:9000/user/talentum/project_logs/raw/Hadoop/Hadoop_2k.log|\n",
      "|2015-10-18 18:01:50,353|INFO |main  |org.apache.hadoop.mapreduce.v2.app.MRAppMaster|OutputCommitter set in config null                                                                                                                    |2026-02-07 18:56:49.691|hdfs://localhost:9000/user/talentum/project_logs/raw/Hadoop/Hadoop_2k.log|\n",
      "+-----------------------+-----+------+----------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Writing to /user/talentum/project_logs/refined/Hadoop...\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION FOR THIS RUN ---\n",
    "# Change this to: \"Android\", \"Linux\", \"Apache\", etc. to test different logs\n",
    "TARGET_SOURCE = \"Hadoop\" \n",
    "\n",
    "# 1. Initialize Spark & Config\n",
    "conf = load_config()\n",
    "spark = get_spark_session(f\"Notebook_Ingest_{TARGET_SOURCE}\")\n",
    "\n",
    "print(f\"--- Processing Source: {TARGET_SOURCE} ---\")\n",
    "\n",
    "# 2. Get Paths\n",
    "raw_path = f\"{conf['storage']['raw']}/{TARGET_SOURCE}\"\n",
    "refined_path = f\"{conf['storage']['refined']}/{TARGET_SOURCE}\"\n",
    "\n",
    "# 3. Get Schema (Regex)\n",
    "if TARGET_SOURCE not in schemas.LOG_PATTERNS:\n",
    "    raise ValueError(f\"Error: No Regex found for {TARGET_SOURCE} in schemas.py\")\n",
    "\n",
    "log_def = schemas.LOG_PATTERNS[TARGET_SOURCE]\n",
    "pattern = log_def[\"pattern\"]\n",
    "columns = log_def[\"columns\"]\n",
    "\n",
    "print(f\"Reading from: {raw_path}\")\n",
    "print(f\"Pattern: {pattern}\")\n",
    "\n",
    "# 4. Read Data\n",
    "try:\n",
    "    raw_df = spark.read.text(raw_path)\n",
    "    print(f\"Raw Count: {raw_df.count()} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    # Stop execution of this cell if file not found\n",
    "    raise e\n",
    "\n",
    "# 5. Apply Regex & Parse\n",
    "parsed_df = raw_df\n",
    "for idx, col_name in enumerate(columns):\n",
    "    parsed_df = parsed_df.withColumn(col_name, regexp_extract(col(\"value\"), pattern, idx + 1))\n",
    "\n",
    "# Filter out empty rows (where regex didn't match)\n",
    "clean_df = parsed_df.filter(col(columns[0]) != \"\")\n",
    "\n",
    "# 6. Add Audit Columns\n",
    "final_df = clean_df \\\n",
    "    .withColumn(\"ingest_time\", current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", input_file_name()) \\\n",
    "    .drop(\"value\")\n",
    "\n",
    "# 7. VISUALIZE (The benefit of Notebooks!)\n",
    "print(\"--- Data Preview ---\")\n",
    "final_df.show(5, truncate=False)\n",
    "\n",
    "# 8. Write to Disk\n",
    "print(f\"Writing to {refined_path}...\")\n",
    "final_df.write.mode(\"overwrite\").parquet(refined_path)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482277cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
