{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6ba09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Environment is ready.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ====================================================\n",
    "# SETUP: AUTO-DETECT SPARK & PROJECT PATH\n",
    "# ====================================================\n",
    "\n",
    "# 1. Detect Spark on the VM (Fix for \"ModuleNotFoundError\")\n",
    "candidate_paths = [\n",
    "    os.environ.get(\"SPARK_HOME\"), \n",
    "    \"/usr/local/spark\",\n",
    "    \"/usr/lib/spark\",\n",
    "    \"/home/talentum/spark\",\n",
    "    \"/opt/spark\"\n",
    "]\n",
    "\n",
    "SPARK_HOME = None\n",
    "for path in candidate_paths:\n",
    "    if path and os.path.exists(path) and os.path.exists(os.path.join(path, \"python\")):\n",
    "        SPARK_HOME = path\n",
    "        break\n",
    "\n",
    "if SPARK_HOME:\n",
    "    sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))\n",
    "    py4j_files = glob.glob(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-*-src.zip\"))\n",
    "    if py4j_files:\n",
    "        sys.path.insert(0, py4j_files[0])\n",
    "else:\n",
    "    print(\"Warning: Spark folder not found. Relying on default environment.\")\n",
    "\n",
    "# 2. Add Project Root to Path (Fix for \"config\" import)\n",
    "PROJECT_ROOT = \"/home/talentum/Distributed-log-analyzer\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# 3. Import Dependencies\n",
    "try:\n",
    "    from pyspark.sql.functions import col, to_timestamp, window, count, concat, lit, when\n",
    "    from spark_jobs.common.spark_utils import get_spark_session, load_config\n",
    "    print(\"SUCCESS: Environment is ready.\")\n",
    "except ImportError as e:\n",
    "    print(f\"FAILURE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Trends for: Hadoop ---\n",
      "Reading from: /user/talentum/project_logs/refined/Hadoop\n",
      "Loaded 2000 records.\n",
      "Standardizing Timestamps...\n",
      "--- Hourly Trends Preview ---\n",
      "+-------------------+----------+-----------+----------+\n",
      "|window_start       |total_logs|error_count|warn_count|\n",
      "+-------------------+----------+-----------+----------+\n",
      "|2015-10-18 17:30:00|2000      |152        |808       |\n",
      "+-------------------+----------+-----------+----------+\n",
      "\n",
      "Writing Gold data to /user/talentum/project_logs/curated/Hadoop_trends...\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Change this to \"HDFS\", \"Android\", etc. to process other logs\n",
    "SOURCE_NAME = \"Hadoop\" \n",
    "\n",
    "# 1. Initialize Spark\n",
    "conf = load_config()\n",
    "spark = get_spark_session(f\"Notebook_Trends_{SOURCE_NAME}\")\n",
    "\n",
    "# 2. Define Paths\n",
    "silver_path = f\"{conf['storage']['refined']}/{SOURCE_NAME}\"\n",
    "gold_path = f\"{conf['storage']['curated']}/{SOURCE_NAME}_trends\"\n",
    "\n",
    "print(f\"--- Processing Trends for: {SOURCE_NAME} ---\")\n",
    "print(f\"Reading from: {silver_path}\")\n",
    "\n",
    "# 3. Read Silver Data\n",
    "try:\n",
    "    df = spark.read.parquet(silver_path)\n",
    "    print(f\"Loaded {df.count()} records.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not read data. Did you run Ingestion first? {e}\")\n",
    "    # Stop execution if data is missing\n",
    "    raise e\n",
    "\n",
    "# 4. Standardize Timestamp\n",
    "# We need to normalize different log formats into one standard \"Timestamp\" object\n",
    "print(\"Standardizing Timestamps...\")\n",
    "\n",
    "if SOURCE_NAME == \"Hadoop\":\n",
    "    # Format: 2015-10-18 18:01:47,978\n",
    "    df_clean = df.withColumn(\"fixed_timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss,SSS\"))\n",
    "\n",
    "elif SOURCE_NAME == \"HDFS\":\n",
    "    # HDFS has separate date/time columns: 081109 203615\n",
    "    df_clean = df.withColumn(\"ts_str\", concat(col(\"date\"), lit(\" \"), col(\"time\"))) \\\n",
    "                 .withColumn(\"fixed_timestamp\", to_timestamp(col(\"ts_str\"), \"yyMMdd HHmmss\"))\n",
    "\n",
    "elif SOURCE_NAME == \"Android\":\n",
    "    # Android has no year: 03-17 16:13:38.811 -> Assume 2023\n",
    "    df_clean = df.withColumn(\"fixed_timestamp\", to_timestamp(concat(lit(\"2023-\"), col(\"timestamp\")), \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "\n",
    "elif SOURCE_NAME in [\"Linux\", \"Mac\", \"OpenSSH\"]:\n",
    "    # Syslog format: Jun 14 15:16:01 -> Assume 2023\n",
    "    df_clean = df.withColumn(\"fixed_timestamp\", to_timestamp(concat(lit(\"2023 \"), col(\"timestamp_str\")), \"yyyy MMM dd HH:mm:ss\"))\n",
    "\n",
    "else:\n",
    "    # Fallback for others (Apache, Zookeeper, etc.)\n",
    "    # Attempt to cast existing timestamp column\n",
    "    df_clean = df.withColumn(\"fixed_timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Filter out any rows where timestamp failed to parse\n",
    "df_clean = df_clean.filter(col(\"fixed_timestamp\").isNotNull())\n",
    "\n",
    "# 5. AGGREGATION (The Gold Layer Logic)\n",
    "# Group by 1-hour windows and count errors\n",
    "trends_df = df_clean.groupBy(window(col(\"fixed_timestamp\"), \"1 hour\").alias(\"time_window\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_logs\"),\n",
    "        count(when(col(\"level\").isin(\"ERROR\", \"FATAL\", \"CRITICAL\"), True)).alias(\"error_count\"),\n",
    "        count(when(col(\"level\") == \"WARN\", True)).alias(\"warn_count\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"time_window.start\").alias(\"window_start\"),\n",
    "        col(\"total_logs\"),\n",
    "        col(\"error_count\"),\n",
    "        col(\"warn_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"window_start\")\n",
    "\n",
    "# 6. Preview & Save\n",
    "print(\"--- Hourly Trends Preview ---\")\n",
    "trends_df.show(10, truncate=False)\n",
    "\n",
    "print(f\"Writing Gold data to {gold_path}...\")\n",
    "trends_df.write.mode(\"overwrite\").parquet(gold_path)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dcc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
