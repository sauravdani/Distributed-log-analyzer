{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09bbdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Environment is ready. You can now run the next cell.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "# ====================================================\n",
    "# SETUP: AUTO-DETECT SPARK & PROJECT PATH\n",
    "# ====================================================\n",
    "\n",
    "# 1. Detect Spark on the VM (Fix for \"ModuleNotFoundError\")\n",
    "# We look in common installation directories\n",
    "candidate_paths = [\n",
    "    os.environ.get(\"SPARK_HOME\"), \n",
    "    \"/usr/local/spark\",\n",
    "    \"/usr/lib/spark\",\n",
    "    \"/home/talentum/spark\",\n",
    "    \"/opt/spark\"\n",
    "]\n",
    "\n",
    "SPARK_HOME = None\n",
    "for path in candidate_paths:\n",
    "    if path and os.path.exists(path) and os.path.exists(os.path.join(path, \"python\")):\n",
    "        SPARK_HOME = path\n",
    "        break\n",
    "\n",
    "if SPARK_HOME:\n",
    "    # Add spark/python to the system path so we can import pyspark\n",
    "    sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))\n",
    "    \n",
    "    # Add py4j (required for Java communication)\n",
    "    py4j_files = glob.glob(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-*-src.zip\"))\n",
    "    if py4j_files:\n",
    "        sys.path.insert(0, py4j_files[0])\n",
    "else:\n",
    "    print(\"Warning: Spark folder not found. Relying on default environment.\")\n",
    "\n",
    "# 2. Add Project Root to Path (Fix for \"config\" import)\n",
    "PROJECT_ROOT = \"/home/talentum/Distributed-log-analyzer\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# 3. Test Imports\n",
    "try:\n",
    "    from pyspark.sql.functions import col, mean, stddev, abs as spark_abs, when, lit\n",
    "    from spark_jobs.common.spark_utils import get_spark_session, load_config\n",
    "    print(\"SUCCESS: Environment is ready. You can now run the next cell.\")\n",
    "except ImportError as e:\n",
    "    print(f\"FAILURE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f90fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Gold Data from: /user/talentum/project_logs/curated/Hadoop_trends\n",
      "Loaded 1 hourly records.\n",
      "\n",
      "--- DETECTED ANOMALIES (Spikes) ---\n",
      "Good news! No anomalies detected. System is stable.\n",
      "\n",
      "--- Recent Data (Top 5 Normal Hours) ---\n",
      "+-------------------+----------+-----------+----------+-------+----------+\n",
      "|       window_start|total_logs|error_count|warn_count|z_score|is_anomaly|\n",
      "+-------------------+----------+-----------+----------+-------+----------+\n",
      "|2015-10-18 17:30:00|      2000|        152|       808|   null|     false|\n",
      "+-------------------+----------+-----------+----------+-------+----------+\n",
      "\n",
      "Saving anomalies to /user/talentum/project_logs/curated/Hadoop_anomalies...\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "SOURCE_NAME = \"Hadoop\" # Change this to analyze other logs later\n",
    "\n",
    "# 1. Imports\n",
    "from pyspark.sql.functions import col, mean, stddev, abs as spark_abs, when, lit\n",
    "from spark_jobs.common.spark_utils import get_spark_session, load_config\n",
    "\n",
    "# 2. Initialize\n",
    "conf = load_config()\n",
    "spark = get_spark_session(f\"Notebook_Anomaly_{SOURCE_NAME}\")\n",
    "\n",
    "# 3. Define Paths\n",
    "gold_path = f\"{conf['storage']['curated']}/{SOURCE_NAME}_trends\"\n",
    "alert_output_path = f\"{conf['storage']['curated']}/{SOURCE_NAME}_anomalies\"\n",
    "\n",
    "print(f\"Reading Gold Data from: {gold_path}\")\n",
    "\n",
    "# 4. Load the Trend Data\n",
    "try:\n",
    "    df = spark.read.parquet(gold_path)\n",
    "    # Cache it because we will use it multiple times (stats + filtering)\n",
    "    df.cache()\n",
    "    print(f\"Loaded {df.count()} hourly records.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not read Gold data. Did you run the Trend Analysis step? {e}\")\n",
    "    raise e\n",
    "    \n",
    "\n",
    "# 5. CALCULATE STATISTICS\n",
    "stats = df.select(\n",
    "    mean(col(\"error_count\")).alias(\"avg_error\"),\n",
    "    stddev(col(\"error_count\")).alias(\"std_error\")\n",
    ").collect()[0]\n",
    "\n",
    "avg_error = stats[\"avg_error\"]\n",
    "std_error = stats[\"std_error\"]\n",
    "\n",
    "# --- FIX: Handle NaN (Not a Number) ---\n",
    "# If we only have 1 row, std_error is None/NaN. We treat it as 0.\n",
    "if std_error is None or math.isnan(std_error):\n",
    "    std_error = 0\n",
    "\n",
    "# Prevent division by zero if std_dev is 0 (extremely stable system)\n",
    "if std_error == 0:\n",
    "    # If std_dev is 0, we use a fallback logic:\n",
    "    # If error > avg, it's an infinite anomaly, otherwise it's 0.\n",
    "    df_analyzed = df.withColumn(\"z_score\", \n",
    "        when(col(\"error_count\") > avg_error, 99.0) # Artificial high score\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "else:\n",
    "    # Standard Z-Score Formula\n",
    "    df_analyzed = df.withColumn(\"z_score\", (col(\"error_count\") - avg_error) / std_error)\n",
    "\n",
    "# Set Threshold\n",
    "df_analyzed = df_analyzed.withColumn(\"is_anomaly\", when(col(\"z_score\") > 3.0, True).otherwise(False))\n",
    "\n",
    "# stats = df.select(\n",
    "#     mean(col(\"error_count\")).alias(\"avg_error\"),\n",
    "#     stddev(col(\"error_count\")).alias(\"std_error\")\n",
    "# ).collect()[0]\n",
    "\n",
    "# avg_error = stats[\"avg_error\"]\n",
    "# std_error = stats[\"std_error\"]\n",
    "\n",
    "# # Handle case where standard deviation is 0 (e.g., all errors are 0)\n",
    "# if std_error is None or std_error == 0:\n",
    "#     std_error = 1  # Avoid division by zero\n",
    "\n",
    "# print(f\"--- Baseline Stats ---\")\n",
    "# print(f\"Average Errors per Hour: {avg_error:.2f}\")\n",
    "# print(f\"Standard Deviation:      {std_error:.2f}\")\n",
    "\n",
    "# 6. DETECT ANOMALIES (Z-Score Method)\n",
    "# Formula: Z = (Current_Value - Average) / StdDev\n",
    "# If Z > 3, it means the value is in the top 0.3% of weirdness (Anomaly)\n",
    "THRESHOLD = 3.0\n",
    "\n",
    "df_analyzed = df.withColumn(\"z_score\", (col(\"error_count\") - avg_error) / std_error) \\\n",
    "                .withColumn(\"is_anomaly\", when(col(\"z_score\") > THRESHOLD, True).otherwise(False))\n",
    "\n",
    "# 7. FILTER & DISPLAY ANOMALIES\n",
    "anomalies_df = df_analyzed.filter(col(\"is_anomaly\") == True) \\\n",
    "    .select(\n",
    "        col(\"window_start\"), \n",
    "        col(\"total_logs\"), \n",
    "        col(\"error_count\"), \n",
    "        col(\"z_score\"),\n",
    "        lit(f\"Error count is {THRESHOLD}x higher than normal\").alias(\"alert_msg\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"z_score\").desc())\n",
    "\n",
    "print(\"\\n--- DETECTED ANOMALIES (Spikes) ---\")\n",
    "if anomalies_df.count() > 0:\n",
    "    anomalies_df.show(truncate=False)\n",
    "else:\n",
    "    print(\"Good news! No anomalies detected. System is stable.\")\n",
    "\n",
    "# 8. OPTIONAL: VISUALIZE \"NORMAL\" DATA vs ANOMALIES\n",
    "print(\"\\n--- Recent Data (Top 5 Normal Hours) ---\")\n",
    "df_analyzed.filter(col(\"is_anomaly\") == False).orderBy(\"window_start\").show(5)\n",
    "\n",
    "# 9. Save Results\n",
    "print(f\"Saving anomalies to {alert_output_path}...\")\n",
    "anomalies_df.write.mode(\"overwrite\").parquet(alert_output_path)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716c118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
